{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tkinter import _flatten\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import benchmark_common as bcommon\n",
    "import config as cfg\n",
    "import os\n",
    "import h5py\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import GRU, Bidirectional\n",
    "from tools import Attention\n",
    "from tkinter import _flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_task1_train = pd.read_feather(cfg.FILE_TASK1_TRAIN)\n",
    "data_task2_train = pd.read_feather(cfg.FILE_TASK2_TRAIN)\n",
    "data_task3_train = pd.read_feather(cfg.FILE_TASK3_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4854"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(_flatten([ item.split(',') for item in  data_task3_train.ec_number.values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>functionCounts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P00958</td>\n",
       "      <td>MSFLISFDKSKKHPAHLQLANNLKIALALEYASKNLKPEVDNDNAA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P00812</td>\n",
       "      <td>METGPHYNYYKNRELSIVLAPFSGGQGKLGVEKGPKYMLKHGLQTS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P00959</td>\n",
       "      <td>MTQVAKKILVTCALPYANGSIHLGHMLEHIQADVWVRYQRMRGHEV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P00348</td>\n",
       "      <td>MAFATRQLVRSLSSSSTAAASAKKILVKHVTVIGGGLMGAGIAQVA...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P00469</td>\n",
       "      <td>MLEQPYLDLAKKVLDEGHFKPDRTHTGTYSIFGHQMRFDLSKGFPL...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222562</th>\n",
       "      <td>Q8I6K2</td>\n",
       "      <td>MSNTAVLNDLVALYDRPTEPMFRVKAKKSFKVPKEYVTDRFKNVAV...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222563</th>\n",
       "      <td>O81103</td>\n",
       "      <td>MATAPSPTTMGTYSSLISTNSFSTFLPNKSQLSLSGKSKHYVARRS...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222564</th>\n",
       "      <td>Q21221</td>\n",
       "      <td>MSSGAPSGSSMSSTPGSPPPRAGGPNSVSFKDLCCLFCCPPFPSSI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222565</th>\n",
       "      <td>Q6QJ72</td>\n",
       "      <td>MSRLLLPKLFSISRTQVPAASLFNNLYRRHKRFVHWTSKMSTDSVR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222566</th>\n",
       "      <td>D9XDR8</td>\n",
       "      <td>MAKMSTTHEEIALAGPDGIPAVDLRDLIDAQLYMPFPFERNPHASE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222567 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                                seq  \\\n",
       "0       P00958  MSFLISFDKSKKHPAHLQLANNLKIALALEYASKNLKPEVDNDNAA...   \n",
       "1       P00812  METGPHYNYYKNRELSIVLAPFSGGQGKLGVEKGPKYMLKHGLQTS...   \n",
       "2       P00959  MTQVAKKILVTCALPYANGSIHLGHMLEHIQADVWVRYQRMRGHEV...   \n",
       "3       P00348  MAFATRQLVRSLSSSSTAAASAKKILVKHVTVIGGGLMGAGIAQVA...   \n",
       "4       P00469  MLEQPYLDLAKKVLDEGHFKPDRTHTGTYSIFGHQMRFDLSKGFPL...   \n",
       "...        ...                                                ...   \n",
       "222562  Q8I6K2  MSNTAVLNDLVALYDRPTEPMFRVKAKKSFKVPKEYVTDRFKNVAV...   \n",
       "222563  O81103  MATAPSPTTMGTYSSLISTNSFSTFLPNKSQLSLSGKSKHYVARRS...   \n",
       "222564  Q21221  MSSGAPSGSSMSSTPGSPPPRAGGPNSVSFKDLCCLFCCPPFPSSI...   \n",
       "222565  Q6QJ72  MSRLLLPKLFSISRTQVPAASLFNNLYRRHKRFVHWTSKMSTDSVR...   \n",
       "222566  D9XDR8  MAKMSTTHEEIALAGPDGIPAVDLRDLIDAQLYMPFPFERNPHASE...   \n",
       "\n",
       "        functionCounts  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  \n",
       "...                ...  \n",
       "222562               1  \n",
       "222563               1  \n",
       "222564               1  \n",
       "222565               1  \n",
       "222566               1  \n",
       "\n",
       "[222567 rows x 3 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_task2_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "enc= preprocessing.OneHotEncoder(sparse=False)\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak=enc.fit_transform([[item] for item in data_task1_train.isenzyme.to_list()])\n",
    "ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ak=enc.fit_transform([[item] for item in data_task2_train.functionCounts.to_list()])\n",
    "ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.inverse_transform([[0,0,0,0,0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.fit_transform(data_task2_train.functionCounts.astype('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 ... 0 0 1]]\n",
      "[('1.-.-.-', '1.1.-.-', '6.6.1.2')]\n"
     ]
    }
   ],
   "source": [
    "label_list = data_task3_train.ec_number.to_list()\n",
    "aa=mlb.fit_transform([ item.split(',') for item in label_list])\n",
    "bb=mlb.transform([['1.-.-.-', '1.1.-.-', '6.6.1.2']])\n",
    "\n",
    "cc=mlb.inverse_transform(bb)\n",
    "print(bb)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]]\n",
      "[(0, 1)]\n"
     ]
    }
   ],
   "source": [
    "label_list = data_task1_train.isenzyme.astype('int').to_list()\n",
    "aa=mlb.fit_transform([label_list])\n",
    "bb=mlb.transform([[0,1]])\n",
    "\n",
    "cc=mlb.inverse_transform(bb)\n",
    "print(bb)\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " ...]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seq</th>\n",
       "      <th>isenzyme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q3J1A3</td>\n",
       "      <td>MADKSDLGYTGLTDEQAQELHSVYMSGLWLFSAVAIVAHLAVYIWRPWF</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P02157</td>\n",
       "      <td>MGLSDGEWQLVLNVWGKVEADLAGHGQEVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P02178</td>\n",
       "      <td>MVLSDAEWQLVLNIWAKVEADVAGHGQDILIRLFKGHPETLEKFDK...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P02194</td>\n",
       "      <td>MGLSDGEWQLVLNIWGKVETDEGGHGKDVLIRLFKGHPETLEKFDK...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P01915</td>\n",
       "      <td>MVWLPRVPCVAAVILLLTVLSPPVALVRDTRPRFLEYVTSECHFYN...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469129</th>\n",
       "      <td>Q21221</td>\n",
       "      <td>MSSGAPSGSSMSSTPGSPPPRAGGPNSVSFKDLCCLFCCPPFPSSI...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469130</th>\n",
       "      <td>Q6QJ72</td>\n",
       "      <td>MSRLLLPKLFSISRTQVPAASLFNNLYRRHKRFVHWTSKMSTDSVR...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469131</th>\n",
       "      <td>C0HL68</td>\n",
       "      <td>GLFSKPAGKGIKNLIPKGVKHIGKEVGKDVIRTGIDVAGCKIKGEC</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469132</th>\n",
       "      <td>C0HK74</td>\n",
       "      <td>GSICLEPKVVGPCTAYFPRFYFDSETGKCTPFIYGGCEGNGNNFET...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469133</th>\n",
       "      <td>D9XDR8</td>\n",
       "      <td>MAKMSTTHEEIALAGPDGIPAVDLRDLIDAQLYMPFPFERNPHASE...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>469134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                                seq  isenzyme\n",
       "0       Q3J1A3  MADKSDLGYTGLTDEQAQELHSVYMSGLWLFSAVAIVAHLAVYIWRPWF     False\n",
       "1       P02157  MGLSDGEWQLVLNVWGKVEADLAGHGQEVLIRLFKGHPETLEKFDK...     False\n",
       "2       P02178  MVLSDAEWQLVLNIWAKVEADVAGHGQDILIRLFKGHPETLEKFDK...     False\n",
       "3       P02194  MGLSDGEWQLVLNIWGKVETDEGGHGKDVLIRLFKGHPETLEKFDK...     False\n",
       "4       P01915  MVWLPRVPCVAAVILLLTVLSPPVALVRDTRPRFLEYVTSECHFYN...     False\n",
       "...        ...                                                ...       ...\n",
       "469129  Q21221  MSSGAPSGSSMSSTPGSPPPRAGGPNSVSFKDLCCLFCCPPFPSSI...      True\n",
       "469130  Q6QJ72  MSRLLLPKLFSISRTQVPAASLFNNLYRRHKRFVHWTSKMSTDSVR...      True\n",
       "469131  C0HL68     GLFSKPAGKGIKNLIPKGVKHIGKEVGKDVIRTGIDVAGCKIKGEC     False\n",
       "469132  C0HK74  GSICLEPKVVGPCTAYFPRFYFDSETGKCTPFIYGGCEGNGNNFET...     False\n",
       "469133  D9XDR8  MAKMSTTHEEIALAGPDGIPAVDLRDLIDAQLYMPFPFERNPHASE...      True\n",
       "\n",
       "[469134 rows x 3 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_task1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad input shape (1, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m lb_encoded \u001b[39m=\u001b[39m encoder_label\u001b[39m.\u001b[39mfit_transform(\u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(_flatten([item\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m label_list]))))\n\u001b[1;32m      6\u001b[0m encoder_onehot\u001b[39m.\u001b[39mfit(lb_encoded\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m onehot_encoded_lb \u001b[39m=\u001b[39m encoder_onehot\u001b[39m.\u001b[39mtransform(encoder_label\u001b[39m.\u001b[39;49mtransform([(\u001b[39m'\u001b[39;49m\u001b[39m1.1.1.-\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m1.21.-.-\u001b[39;49m\u001b[39m'\u001b[39;49m)])\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[39m# if save ==True:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#     joblib.dump(encoder_onehot, file_enc_onehot)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#     joblib.dump(encoder_label, file_enc_lb)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m onehot_encoded_lb\u001b[39m.\u001b[39mA\n",
      "File \u001b[0;32m~/miniconda3/envs/DMLF/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:268\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform labels to normalized encoding.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39my : array-like of shape [n_samples]\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    267\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    269\u001b[0m \u001b[39m# transform of empty array is empty array\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m _num_samples(y) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/DMLF/lib/python3.8/site-packages/sklearn/utils/validation.py:781\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[0;34m(y, warn)\u001b[0m\n\u001b[1;32m    775\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39m(n_samples, ), for example using ravel().\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    778\u001b[0m                       DataConversionWarning, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    779\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mravel(y)\n\u001b[0;32m--> 781\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbad input shape \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape))\n",
      "\u001b[0;31mValueError\u001b[0m: bad input shape (1, 2)"
     ]
    }
   ],
   "source": [
    "label_list = data_task3_train.ec_number.to_list()\n",
    "\n",
    "encoder_onehot = preprocessing.OneHotEncoder()\n",
    "encoder_label = preprocessing.LabelEncoder()\n",
    "lb_encoded = encoder_label.fit_transform(list(set(_flatten([ item.split(',') for item in label_list]))))\n",
    "encoder_onehot.fit(lb_encoded.reshape(-1,1))\n",
    "onehot_encoded_lb = encoder_onehot.transform(encoder_label.transform(label_list).reshape(-1, 1))\n",
    "\n",
    "# if save ==True:\n",
    "#     joblib.dump(encoder_onehot, file_enc_onehot)\n",
    "#     joblib.dump(encoder_label, file_enc_lb)\n",
    "\n",
    "onehot_encoded_lb.A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5408</th>\n",
       "      <th>5409</th>\n",
       "      <th>5410</th>\n",
       "      <th>5411</th>\n",
       "      <th>5412</th>\n",
       "      <th>5413</th>\n",
       "      <th>5414</th>\n",
       "      <th>5415</th>\n",
       "      <th>5416</th>\n",
       "      <th>5417</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222562</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222563</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>222567 rows × 5418 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9     ...  5408  \\\n",
       "0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "222562   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "222563   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "222564   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "222565   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "222566   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "        5409  5410  5411  5412  5413  5414  5415  5416  5417  \n",
       "0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "222562   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "222563   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "222564   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "222565   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "222566   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[222567 rows x 5418 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(onehot_encoded_lb.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_onehot_label(label_list, save=True, file_enc_lb='', file_enc_onehot=''):\n",
    "    encoder_onehot = preprocessing.OneHotEncoder()\n",
    "    encoder_label = preprocessing.LabelEncoder()\n",
    "    lb_encoded = encoder_label.fit_transform(label_list)\n",
    "    encoder_onehot.fit(lb_encoded.reshape(-1,1))\n",
    "    onehot_encoded_lb = encoder_onehot.transform(encoder_label.transform(label_list).reshape(-1, 1))\n",
    "\n",
    "    if save ==True:\n",
    "        joblib.dump(encoder_onehot, file_enc_onehot)\n",
    "        joblib.dump(encoder_label, file_enc_lb)\n",
    "\n",
    "    return onehot_encoded_lb.A\n",
    "    \n",
    "\n",
    "make_onehot_label(label_list=['a','c','d'], file_enc_lb='task1_lbenc_lb.h5', file_enc_onehot='task1_lbenc_onehot.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "  (0, 0)\t1.0\n",
      "  (1, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "lb = LabelEncoder()\n",
    "tmp = lb.fit_transform([123,456,789])\n",
    "print(tmp)#输出LabelEncoder的结果\n",
    "enc.fit(tmp.reshape(-1,1))#将LabelEncoder的结果作为OneHotEncoder特征输入\n",
    "x_train = enc.transform(lb.transform([123,789]).reshape(-1, 1))\n",
    "#输出特征[123,789]的OneHotEncoder的编码结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "               #关闭文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123, 789])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.inverse_transform(enc.inverse_transform(x_train).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa.h5']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(lb,'aa.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk=joblib.load('aa.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123, 789])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk.inverse_transform(enc.inverse_transform(x_train).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(x_train.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform([1, 1, 2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 6])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform([0, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabelEncoder()\n",
    "le.classes_\n",
    "array([1, 2, 6])\n",
    "le.transform([1, 1, 2, 6])\n",
    "array([0, 0, 1, 2])\n",
    "le.inverse_transform([0, 0, 1, 2])\n",
    "array([1, 1, 2, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region\n",
    "def get_train_X_Y(traindata, feature_bankfile, task=1, task3_test=None):\n",
    "    \n",
    "    traindata = traindata.merge(feature_bankfile, on='id', how='left')\n",
    "    return traindata\n",
    "#endregion\n",
    "\n",
    "\n",
    "\n",
    "#region 获取EC号训练的数据集\n",
    "def get_ec_train_set(train_data, ec_label_dict):\n",
    "    \"\"\"[获取EC号训练的数据集]\n",
    "    Args:\n",
    "        ec_label_dict: ([dict]): [ec to label dict]\n",
    "        train_data ([DataFrame]): [description]\n",
    "    Returns:\n",
    "        [DataFrame]: [[train_x, trian_y]]\n",
    "    \"\"\"\n",
    "    if cfg.TRAIN_USE_ONLY_ENZYME:\n",
    "        train_data = train_data[train_data.isemzyme] #仅选用酶数据\n",
    "    # if cfg.TRAIN_USE_ONLY_SINGLE_FUNCTION:\n",
    "    train_data = train_data[train_data.functionCounts ==1] #仅选用单功能酶数据\n",
    "    \n",
    "    train_data = train_data[(train_data.ec_specific_level >= cfg.TRAIN_USE_SPCIFIC_EC_LEVEL) |(train_data.ec_specific_level ==0)]\n",
    "    train_data.reset_index(drop=True, inplace=True)\n",
    "    train_data.insert(loc=1, column='ec_label', value=train_data.ec_number.apply(lambda x: ec_label_dict.get(x)))\n",
    "    # train_data['ec_label'] = train_data.ec_number.apply(lambda x: ec_label_dict.get(x))\n",
    "\n",
    "    train_X = train_data.iloc[:, 8:]\n",
    "    train_Y =train_data['ec_label']\n",
    "    return train_X, pd.DataFrame(train_Y)\n",
    "\n",
    "#endregion\n",
    "\n",
    "#region 训练是否是酶模型\n",
    "def train_isenzyme(x_train,y_train,x_vali,y_vali,ecs, model_file,force_model_update=False):\n",
    "    \"\"\"[训练是否是酶模型]\n",
    "\n",
    "    Args:\n",
    "        vali_ratio:\n",
    "        X ([DataFrame]): [特征数据]\n",
    "        Y ([DataFrame]): [标签数据]\n",
    "        model_file ([string]): [模型的存放路径]\n",
    "        force_model_update (bool, optional): [是否强制更新模型]. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        [object]: [训练好的模型]\n",
    "    \"\"\"\n",
    "    if os.path.exists(model_file) and (force_model_update==False):\n",
    "        return\n",
    "    else:\n",
    "        # 模型设置，训练，保存\n",
    "        inputs = Input(shape=(1,x_train.shape[2]), name=\"input\")\n",
    "        gru = Bidirectional(GRU(512, dropout=0.2, return_sequences=True), name=\"bi-gru\")(inputs)\n",
    "        attention = Attention.Attention(32)(gru)\n",
    "        num_class = len(ecs)\n",
    "        output = Dense(num_class, activation='sigmoid', name=\"dense\")(attention)\n",
    "        model = Model(inputs, output)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "        model.fit(x_train, y_train, validation_data=(x_vali,y_vali), batch_size=512, epochs= 400)\n",
    "        # 保存\n",
    "        model.save(model_file)\n",
    "        print('gru+attention模型训练完成')\n",
    "        return model\n",
    "#endregion\n",
    "\n",
    "#region 构建几功能酶模型\n",
    "def train_howmany_enzyme(x_train,y_train,x_vali,y_vali,ecs, model_file,force_model_update=False):\n",
    "    \"\"\"[构建几功能酶模型]\n",
    "\n",
    "    Args:\n",
    "        force_model_update:\n",
    "        vali_ratio:\n",
    "        model_file:\n",
    "        data_x ([DataFrame]): [X训练数据]\n",
    "        data_y ([DataFrame]): [Y训练数据]\n",
    "\n",
    "    Returns:\n",
    "        [object]: [训练好的模型]\n",
    "    \"\"\"\n",
    "    if os.path.exists(model_file) and (force_model_update==False):\n",
    "        return\n",
    "    else:\n",
    "        # 模型设置，训练，保存\n",
    "        inputs = Input(shape=(1,x_train.shape[2]), name=\"input\")\n",
    "        gru = Bidirectional(GRU(512, dropout=0.2, return_sequences=True), name=\"bi-gru\")(inputs)\n",
    "        attention = Attention.Attention(32)(gru)\n",
    "        num_class = len(ecs)\n",
    "        output = Dense(num_class, activation='sigmoid', name=\"dense\")(attention)\n",
    "        model = Model(inputs, output)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=Adam(),\n",
    "                    metrics=['accuracy'])\n",
    "        model.fit(x_train, y_train, validation_data=(x_vali,y_vali),batch_size=512, epochs= 1)\n",
    "        # 保存\n",
    "        model.save(model_file)\n",
    "        print('gru+attention模型训练完成')\n",
    "        return model\n",
    "#endregion\n",
    "\n",
    "\n",
    "def make_ec_label_dict(ec_str_list, file_save=cfg.FILE_EC_LABEL_DICT):\n",
    "    if os.path.exists(file_save):\n",
    "        print(f'ec label dict already exist, using existing file:{cfg.FILE_EC_LABEL_DICT}')\n",
    "        dict_ec_label = np.load(cfg.FILE_EC_LABEL_DICT, allow_pickle=True).item()\n",
    "        return dict_ec_label\n",
    "\n",
    "    ecs = _flatten([item.split(',') for item in ec_str_list])\n",
    "    ecs = list(set(ecs))\n",
    "    ec_label_dict = {k: v for k, v in zip(ecs, range(len(ecs)))}\n",
    "    np.save(file_save, ec_label_dict)\n",
    "    return  ec_label_dict\n",
    "    print('字典保存成功')\n",
    "\n",
    "\n",
    "#region 训练slice模型\n",
    "def train_ec_gru(x_train,y_train,x_vali,y_vali,ecs, model_file,force_model_update=False):\n",
    "    \"\"\"[训练slice模型]\n",
    "    Args:\n",
    "        trainX ([DataFarame]): [X特征]\n",
    "        trainY ([DataFrame]): [ Y标签]]\n",
    "        modelPath ([string]): [存放模型的目录]\n",
    "        force_model_update (bool, optional): [是否强制更新模型]. Defaults to False.\n",
    "    \"\"\"\n",
    "    if os.path.exists(model_file) and (force_model_update==False):\n",
    "        return\n",
    "    else:\n",
    "        # 模型设置，训练，保存\n",
    "        inputs = Input(shape=(1,x_train.shape[2]), name=\"input\")\n",
    "        gru = Bidirectional(GRU(512, dropout=0.2, return_sequences=True), name=\"bi-gru\")(inputs)\n",
    "        attention = Attention.Attention(32)(gru)\n",
    "        num_class = len(ecs)\n",
    "        output = Dense(num_class, activation='sigmoid', name=\"dense\")(attention)\n",
    "        model = Model(inputs, output)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=Adam(),\n",
    "                    metrics=['accuracy'])\n",
    "        model.fit(x_train, y_train, validation_data=(x_vali,y_vali),batch_size=3948, epochs= 400)\n",
    "        # 保存\n",
    "        model.save(model_file)\n",
    "        return model\n",
    "#endregion\n",
    "\n",
    "#region 数据处理\n",
    "def data_process(dataset, task=1,vali_ratio=0.3):\n",
    "    labe = 'isenzyme'\n",
    "    x_train, x_vali = train_test_split(dataset,test_size=vali_ratio,random_state=1)\n",
    "\n",
    "    if task == 1:\n",
    "        label = 'isenzyme'\n",
    "    elif task == 2:\n",
    "        label = 'functionCounts'\n",
    "    else:\n",
    "        label = 'ec_number'\n",
    "    \n",
    "    ecs= (set(dataset[label]))\n",
    "    label_dict = dict(zip(set(ecs), range(len(ecs))))\n",
    "    def get_label(ecnum_str):\n",
    "        label_init = np.zeros(len(label_dict),  dtype=int)\n",
    "        label_init[label_dict.get(ecnum_str)] = 1\n",
    "        return list(label_init)\n",
    "    train_label = x_train[label].apply(lambda x: get_label(ecnum_str=x))\n",
    "    train_label=[item for item in train_label]\n",
    "\n",
    "    vali_label = x_vali[label].apply(lambda x: get_label(ecnum_str=x))\n",
    "    vali_label=[item for item in vali_label]\n",
    "\n",
    "    y_train = np.array(train_label)\n",
    "    y_vali = np.array(vali_label)\n",
    "\n",
    "    x_train = x_train.iloc[:,3:]\n",
    "    x_train = np.array(x_train)\n",
    "    x_train =np.reshape(x_train,(x_train.shape[0],1,x_train.shape[1]))\n",
    "\n",
    "    x_vali = x_vali.iloc[:,3:]\n",
    "    x_vali = np.array(x_vali)\n",
    "    x_vali =np.reshape(x_vali,(x_vali.shape[0],1,x_vali.shape[1]))\n",
    "    \n",
    "    return x_train,y_train,x_vali,y_vali,ecs\n",
    "\n",
    "#endregion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelsave(ecs,labefile):\n",
    "    fileObject = open(labefile, 'w')  \n",
    "    for ip in ecs:  \n",
    "        fileObject.write(str(ip))  \n",
    "        fileObject.write('\\n') \n",
    "    fileObject.close() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 loading task data\n",
      "step 2: Loading features, embdding method=esm32\n",
      "step 3: train isEnzyme model\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_METHOD = 'esm32'\n",
    "\n",
    "# 1. 读入数据\n",
    "print('step 1 loading task data')\n",
    "data_task1_train = pd.read_feather(cfg.FILE_TASK1_TRAIN)\n",
    "data_task2_train = pd.read_feather(cfg.FILE_TASK2_TRAIN)\n",
    "data_task3_train = pd.read_feather(cfg.FILE_TASK3_TRAIN)\n",
    "data_task3_test = pd.read_feather(cfg.FILE_TASK3_TEST_2019)\n",
    "\n",
    "# # 2. 读取特征\n",
    "print(f'step 2: Loading features, embdding method={EMBEDDING_METHOD}')\n",
    "feature_df = bcommon.load_data_embedding(embedding_type=EMBEDDING_METHOD)\n",
    "\n",
    "#3. task1 train\n",
    "print('step 3: train isEnzyme model')\n",
    "task1_X = get_train_X_Y(traindata=data_task1_train, feature_bankfile=feature_df, task=1)\n",
    "task1_X = task1_X.iloc[:100,:]\n",
    "x_train,y_train,x_vali,y_vali,ecs = data_process(task1_X,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 1, 1280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 12:24:09.411750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 24061 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:37:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-02 12:24:13.978446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2023-02-02 12:24:14.142700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 0.6876 - accuracy: 0.6000 - val_loss: 0.1584 - val_accuracy: 1.0000\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1667 - accuracy: 1.0000 - val_loss: 0.0370 - val_accuracy: 1.0000\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0415 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 5.7218e-04 - val_accuracy: 1.0000\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 2.7300e-04 - val_accuracy: 1.0000\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.6554e-04 - accuracy: 1.0000 - val_loss: 1.4155e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.1809e-04 - accuracy: 1.0000 - val_loss: 7.8788e-05 - val_accuracy: 1.0000\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2600e-04 - accuracy: 1.0000 - val_loss: 4.6632e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6350e-04 - accuracy: 1.0000 - val_loss: 2.9118e-05 - val_accuracy: 1.0000\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3454e-04 - accuracy: 1.0000 - val_loss: 1.9057e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.7337e-05 - accuracy: 1.0000 - val_loss: 1.3000e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.6183e-05 - accuracy: 1.0000 - val_loss: 9.2018e-06 - val_accuracy: 1.0000\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8586e-05 - accuracy: 1.0000 - val_loss: 6.7305e-06 - val_accuracy: 1.0000\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.3366e-05 - accuracy: 1.0000 - val_loss: 5.0697e-06 - val_accuracy: 1.0000\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4769e-05 - accuracy: 1.0000 - val_loss: 3.9207e-06 - val_accuracy: 1.0000\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.3572e-05 - accuracy: 1.0000 - val_loss: 3.1048e-06 - val_accuracy: 1.0000\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.2642e-05 - accuracy: 1.0000 - val_loss: 2.5118e-06 - val_accuracy: 1.0000\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4925e-05 - accuracy: 1.0000 - val_loss: 2.0716e-06 - val_accuracy: 1.0000\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4471e-05 - accuracy: 1.0000 - val_loss: 1.7386e-06 - val_accuracy: 1.0000\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0013e-05 - accuracy: 1.0000 - val_loss: 1.4823e-06 - val_accuracy: 1.0000\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.8789e-05 - accuracy: 1.0000 - val_loss: 1.2821e-06 - val_accuracy: 1.0000\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3579e-05 - accuracy: 1.0000 - val_loss: 1.1234e-06 - val_accuracy: 1.0000\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2809e-05 - accuracy: 1.0000 - val_loss: 9.9598e-07 - val_accuracy: 1.0000\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0414e-05 - accuracy: 1.0000 - val_loss: 8.9262e-07 - val_accuracy: 1.0000\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.2848e-06 - accuracy: 1.0000 - val_loss: 8.0782e-07 - val_accuracy: 1.0000\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2854e-05 - accuracy: 1.0000 - val_loss: 7.3758e-07 - val_accuracy: 1.0000\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2047e-05 - accuracy: 1.0000 - val_loss: 6.7892e-07 - val_accuracy: 1.0000\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1508e-05 - accuracy: 1.0000 - val_loss: 6.2953e-07 - val_accuracy: 1.0000\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0120e-05 - accuracy: 1.0000 - val_loss: 5.8766e-07 - val_accuracy: 1.0000\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.0129e-06 - accuracy: 1.0000 - val_loss: 5.5195e-07 - val_accuracy: 1.0000\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.6150e-06 - accuracy: 1.0000 - val_loss: 5.2130e-07 - val_accuracy: 1.0000\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0551e-05 - accuracy: 1.0000 - val_loss: 4.9483e-07 - val_accuracy: 1.0000\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.9657e-06 - accuracy: 1.0000 - val_loss: 4.7189e-07 - val_accuracy: 1.0000\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.3776e-06 - accuracy: 1.0000 - val_loss: 4.5192e-07 - val_accuracy: 1.0000\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.4415e-06 - accuracy: 1.0000 - val_loss: 4.3447e-07 - val_accuracy: 1.0000\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.1542e-06 - accuracy: 1.0000 - val_loss: 4.1913e-07 - val_accuracy: 1.0000\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.8922e-06 - accuracy: 1.0000 - val_loss: 4.0563e-07 - val_accuracy: 1.0000\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.6163e-06 - accuracy: 1.0000 - val_loss: 3.9372e-07 - val_accuracy: 1.0000\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.8383e-06 - accuracy: 1.0000 - val_loss: 3.8317e-07 - val_accuracy: 1.0000\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.8457e-06 - accuracy: 1.0000 - val_loss: 3.7380e-07 - val_accuracy: 1.0000\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.1539e-06 - accuracy: 1.0000 - val_loss: 3.6547e-07 - val_accuracy: 1.0000\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.2545e-06 - accuracy: 1.0000 - val_loss: 3.5803e-07 - val_accuracy: 1.0000\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.4353e-06 - accuracy: 1.0000 - val_loss: 3.5139e-07 - val_accuracy: 1.0000\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.2005e-06 - accuracy: 1.0000 - val_loss: 3.4545e-07 - val_accuracy: 1.0000\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.4661e-06 - accuracy: 1.0000 - val_loss: 3.4012e-07 - val_accuracy: 1.0000\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.6536e-06 - accuracy: 1.0000 - val_loss: 3.3533e-07 - val_accuracy: 1.0000\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.0052e-06 - accuracy: 1.0000 - val_loss: 3.3102e-07 - val_accuracy: 1.0000\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.5008e-06 - accuracy: 1.0000 - val_loss: 3.2714e-07 - val_accuracy: 1.0000\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.0159e-06 - accuracy: 1.0000 - val_loss: 3.2364e-07 - val_accuracy: 1.0000\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.4751e-06 - accuracy: 1.0000 - val_loss: 3.2048e-07 - val_accuracy: 1.0000\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.5111e-06 - accuracy: 1.0000 - val_loss: 3.1761e-07 - val_accuracy: 1.0000\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.9039e-06 - accuracy: 1.0000 - val_loss: 3.1503e-07 - val_accuracy: 1.0000\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.6195e-06 - accuracy: 1.0000 - val_loss: 3.1268e-07 - val_accuracy: 1.0000\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1485e-06 - accuracy: 1.0000 - val_loss: 3.1054e-07 - val_accuracy: 1.0000\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.3285e-06 - accuracy: 1.0000 - val_loss: 3.0861e-07 - val_accuracy: 1.0000\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.9383e-06 - accuracy: 1.0000 - val_loss: 3.0685e-07 - val_accuracy: 1.0000\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6892e-06 - accuracy: 1.0000 - val_loss: 3.0526e-07 - val_accuracy: 1.0000\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.8292e-06 - accuracy: 1.0000 - val_loss: 3.0380e-07 - val_accuracy: 1.0000\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.2823e-06 - accuracy: 1.0000 - val_loss: 3.0246e-07 - val_accuracy: 1.0000\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.0948e-06 - accuracy: 1.0000 - val_loss: 3.0125e-07 - val_accuracy: 1.0000\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.3948e-06 - accuracy: 1.0000 - val_loss: 3.0013e-07 - val_accuracy: 1.0000\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.0191e-06 - accuracy: 1.0000 - val_loss: 2.9911e-07 - val_accuracy: 1.0000\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.2817e-06 - accuracy: 1.0000 - val_loss: 2.9818e-07 - val_accuracy: 1.0000\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.9948e-06 - accuracy: 1.0000 - val_loss: 2.9732e-07 - val_accuracy: 1.0000\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.9765e-06 - accuracy: 1.0000 - val_loss: 2.9653e-07 - val_accuracy: 1.0000\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.5433e-06 - accuracy: 1.0000 - val_loss: 2.9581e-07 - val_accuracy: 1.0000\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.9169e-06 - accuracy: 1.0000 - val_loss: 2.9514e-07 - val_accuracy: 1.0000\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0797e-05 - accuracy: 1.0000 - val_loss: 2.9452e-07 - val_accuracy: 1.0000\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4020e-06 - accuracy: 1.0000 - val_loss: 2.9394e-07 - val_accuracy: 1.0000\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.5732e-06 - accuracy: 1.0000 - val_loss: 2.9340e-07 - val_accuracy: 1.0000\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3473e-06 - accuracy: 1.0000 - val_loss: 2.9291e-07 - val_accuracy: 1.0000\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.0405e-06 - accuracy: 1.0000 - val_loss: 2.9245e-07 - val_accuracy: 1.0000\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.5588e-06 - accuracy: 1.0000 - val_loss: 2.9201e-07 - val_accuracy: 1.0000\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.2182e-06 - accuracy: 1.0000 - val_loss: 2.9160e-07 - val_accuracy: 1.0000\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.9099e-06 - accuracy: 1.0000 - val_loss: 2.9122e-07 - val_accuracy: 1.0000\n",
      "Epoch 78/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.1015e-06 - accuracy: 1.0000 - val_loss: 2.9087e-07 - val_accuracy: 1.0000\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.8697e-06 - accuracy: 1.0000 - val_loss: 2.9053e-07 - val_accuracy: 1.0000\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.2563e-06 - accuracy: 1.0000 - val_loss: 2.9021e-07 - val_accuracy: 1.0000\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.9559e-06 - accuracy: 1.0000 - val_loss: 2.8990e-07 - val_accuracy: 1.0000\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3086e-06 - accuracy: 1.0000 - val_loss: 2.8962e-07 - val_accuracy: 1.0000\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.7822e-06 - accuracy: 1.0000 - val_loss: 2.8933e-07 - val_accuracy: 1.0000\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.6058e-06 - accuracy: 1.0000 - val_loss: 2.8907e-07 - val_accuracy: 1.0000\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.1662e-06 - accuracy: 1.0000 - val_loss: 2.8881e-07 - val_accuracy: 1.0000\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.6454e-06 - accuracy: 1.0000 - val_loss: 2.8856e-07 - val_accuracy: 1.0000\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.2294e-06 - accuracy: 1.0000 - val_loss: 2.8832e-07 - val_accuracy: 1.0000\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.1403e-06 - accuracy: 1.0000 - val_loss: 2.8808e-07 - val_accuracy: 1.0000\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.4380e-06 - accuracy: 1.0000 - val_loss: 2.8785e-07 - val_accuracy: 1.0000\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8110e-06 - accuracy: 1.0000 - val_loss: 2.8764e-07 - val_accuracy: 1.0000\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.5706e-06 - accuracy: 1.0000 - val_loss: 2.8742e-07 - val_accuracy: 1.0000\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.4140e-06 - accuracy: 1.0000 - val_loss: 2.8721e-07 - val_accuracy: 1.0000\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.7556e-06 - accuracy: 1.0000 - val_loss: 2.8700e-07 - val_accuracy: 1.0000\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.5883e-06 - accuracy: 1.0000 - val_loss: 2.8680e-07 - val_accuracy: 1.0000\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.5047e-06 - accuracy: 1.0000 - val_loss: 2.8660e-07 - val_accuracy: 1.0000\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.3846e-06 - accuracy: 1.0000 - val_loss: 2.8641e-07 - val_accuracy: 1.0000\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.8698e-06 - accuracy: 1.0000 - val_loss: 2.8621e-07 - val_accuracy: 1.0000\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3225e-06 - accuracy: 1.0000 - val_loss: 2.8602e-07 - val_accuracy: 1.0000\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.7795e-06 - accuracy: 1.0000 - val_loss: 2.8584e-07 - val_accuracy: 1.0000\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.3565e-06 - accuracy: 1.0000 - val_loss: 2.8565e-07 - val_accuracy: 1.0000\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.6138e-06 - accuracy: 1.0000 - val_loss: 2.8547e-07 - val_accuracy: 1.0000\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.3706e-06 - accuracy: 1.0000 - val_loss: 2.8529e-07 - val_accuracy: 1.0000\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.3289e-06 - accuracy: 1.0000 - val_loss: 2.8512e-07 - val_accuracy: 1.0000\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.4468e-06 - accuracy: 1.0000 - val_loss: 2.8494e-07 - val_accuracy: 1.0000\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6379e-06 - accuracy: 1.0000 - val_loss: 2.8477e-07 - val_accuracy: 1.0000\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.6247e-06 - accuracy: 1.0000 - val_loss: 2.8460e-07 - val_accuracy: 1.0000\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4442e-06 - accuracy: 1.0000 - val_loss: 2.8443e-07 - val_accuracy: 1.0000\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.5223e-06 - accuracy: 1.0000 - val_loss: 2.8427e-07 - val_accuracy: 1.0000\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.4077e-06 - accuracy: 1.0000 - val_loss: 2.8411e-07 - val_accuracy: 1.0000\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.2978e-06 - accuracy: 1.0000 - val_loss: 2.8395e-07 - val_accuracy: 1.0000\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.6251e-06 - accuracy: 1.0000 - val_loss: 2.8378e-07 - val_accuracy: 1.0000\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3909e-06 - accuracy: 1.0000 - val_loss: 2.8362e-07 - val_accuracy: 1.0000\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7865e-06 - accuracy: 1.0000 - val_loss: 2.8346e-07 - val_accuracy: 1.0000\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3473e-06 - accuracy: 1.0000 - val_loss: 2.8330e-07 - val_accuracy: 1.0000\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.9541e-06 - accuracy: 1.0000 - val_loss: 2.8314e-07 - val_accuracy: 1.0000\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.6560e-06 - accuracy: 1.0000 - val_loss: 2.8297e-07 - val_accuracy: 1.0000\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.0722e-06 - accuracy: 1.0000 - val_loss: 2.8281e-07 - val_accuracy: 1.0000\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.1280e-06 - accuracy: 1.0000 - val_loss: 2.8264e-07 - val_accuracy: 1.0000\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.9755e-06 - accuracy: 1.0000 - val_loss: 2.8247e-07 - val_accuracy: 1.0000\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.2579e-06 - accuracy: 1.0000 - val_loss: 2.8230e-07 - val_accuracy: 1.0000\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4589e-06 - accuracy: 1.0000 - val_loss: 2.8214e-07 - val_accuracy: 1.0000\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.8264e-06 - accuracy: 1.0000 - val_loss: 2.8197e-07 - val_accuracy: 1.0000\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.9971e-06 - accuracy: 1.0000 - val_loss: 2.8181e-07 - val_accuracy: 1.0000\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.5639e-06 - accuracy: 1.0000 - val_loss: 2.8165e-07 - val_accuracy: 1.0000\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.5347e-06 - accuracy: 1.0000 - val_loss: 2.8148e-07 - val_accuracy: 1.0000\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.5655e-06 - accuracy: 1.0000 - val_loss: 2.8131e-07 - val_accuracy: 1.0000\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.2389e-06 - accuracy: 1.0000 - val_loss: 2.8115e-07 - val_accuracy: 1.0000\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.7813e-06 - accuracy: 1.0000 - val_loss: 2.8099e-07 - val_accuracy: 1.0000\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.5324e-06 - accuracy: 1.0000 - val_loss: 2.8083e-07 - val_accuracy: 1.0000\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.0106e-06 - accuracy: 1.0000 - val_loss: 2.8067e-07 - val_accuracy: 1.0000\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.5213e-06 - accuracy: 1.0000 - val_loss: 2.8051e-07 - val_accuracy: 1.0000\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.2177e-06 - accuracy: 1.0000 - val_loss: 2.8035e-07 - val_accuracy: 1.0000\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.4958e-06 - accuracy: 1.0000 - val_loss: 2.8018e-07 - val_accuracy: 1.0000\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.3062e-06 - accuracy: 1.0000 - val_loss: 2.8002e-07 - val_accuracy: 1.0000\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3723e-06 - accuracy: 1.0000 - val_loss: 2.7985e-07 - val_accuracy: 1.0000\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.6891e-06 - accuracy: 1.0000 - val_loss: 2.7969e-07 - val_accuracy: 1.0000\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.1798e-06 - accuracy: 1.0000 - val_loss: 2.7952e-07 - val_accuracy: 1.0000\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.5200e-06 - accuracy: 1.0000 - val_loss: 2.7935e-07 - val_accuracy: 1.0000\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.8050e-06 - accuracy: 1.0000 - val_loss: 2.7917e-07 - val_accuracy: 1.0000\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.8902e-06 - accuracy: 1.0000 - val_loss: 2.7900e-07 - val_accuracy: 1.0000\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.3428e-06 - accuracy: 1.0000 - val_loss: 2.7883e-07 - val_accuracy: 1.0000\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.9203e-06 - accuracy: 1.0000 - val_loss: 2.7865e-07 - val_accuracy: 1.0000\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.1076e-06 - accuracy: 1.0000 - val_loss: 2.7848e-07 - val_accuracy: 1.0000\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.5740e-06 - accuracy: 1.0000 - val_loss: 2.7831e-07 - val_accuracy: 1.0000\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.4283e-06 - accuracy: 1.0000 - val_loss: 2.7814e-07 - val_accuracy: 1.0000\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.6050e-06 - accuracy: 1.0000 - val_loss: 2.7796e-07 - val_accuracy: 1.0000\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.8583e-06 - accuracy: 1.0000 - val_loss: 2.7779e-07 - val_accuracy: 1.0000\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.9276e-06 - accuracy: 1.0000 - val_loss: 2.7762e-07 - val_accuracy: 1.0000\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.3622e-06 - accuracy: 1.0000 - val_loss: 2.7744e-07 - val_accuracy: 1.0000\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.5671e-06 - accuracy: 1.0000 - val_loss: 2.7727e-07 - val_accuracy: 1.0000\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.0171e-06 - accuracy: 1.0000 - val_loss: 2.7710e-07 - val_accuracy: 1.0000\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6343e-06 - accuracy: 1.0000 - val_loss: 2.7693e-07 - val_accuracy: 1.0000\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.2394e-06 - accuracy: 1.0000 - val_loss: 2.7675e-07 - val_accuracy: 1.0000\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.5212e-06 - accuracy: 1.0000 - val_loss: 2.7658e-07 - val_accuracy: 1.0000\n",
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.4176e-06 - accuracy: 1.0000 - val_loss: 2.7640e-07 - val_accuracy: 1.0000\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3683e-06 - accuracy: 1.0000 - val_loss: 2.7622e-07 - val_accuracy: 1.0000\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9581e-06 - accuracy: 1.0000 - val_loss: 2.7604e-07 - val_accuracy: 1.0000\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.8767e-06 - accuracy: 1.0000 - val_loss: 2.7586e-07 - val_accuracy: 1.0000\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.7504e-06 - accuracy: 1.0000 - val_loss: 2.7567e-07 - val_accuracy: 1.0000\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6150e-06 - accuracy: 1.0000 - val_loss: 2.7549e-07 - val_accuracy: 1.0000\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.9788e-06 - accuracy: 1.0000 - val_loss: 2.7531e-07 - val_accuracy: 1.0000\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.6296e-06 - accuracy: 1.0000 - val_loss: 2.7513e-07 - val_accuracy: 1.0000\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2573e-06 - accuracy: 1.0000 - val_loss: 2.7494e-07 - val_accuracy: 1.0000\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.9673e-06 - accuracy: 1.0000 - val_loss: 2.7475e-07 - val_accuracy: 1.0000\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5194e-06 - accuracy: 1.0000 - val_loss: 2.7456e-07 - val_accuracy: 1.0000\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.7449e-06 - accuracy: 1.0000 - val_loss: 2.7437e-07 - val_accuracy: 1.0000\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.4509e-06 - accuracy: 1.0000 - val_loss: 2.7417e-07 - val_accuracy: 1.0000\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.4982e-06 - accuracy: 1.0000 - val_loss: 2.7398e-07 - val_accuracy: 1.0000\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.0597e-06 - accuracy: 1.0000 - val_loss: 2.7379e-07 - val_accuracy: 1.0000\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.8564e-06 - accuracy: 1.0000 - val_loss: 2.7360e-07 - val_accuracy: 1.0000\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.0399e-06 - accuracy: 1.0000 - val_loss: 2.7340e-07 - val_accuracy: 1.0000\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.7442e-06 - accuracy: 1.0000 - val_loss: 2.7321e-07 - val_accuracy: 1.0000\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.9409e-06 - accuracy: 1.0000 - val_loss: 2.7302e-07 - val_accuracy: 1.0000\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.8974e-06 - accuracy: 1.0000 - val_loss: 2.7284e-07 - val_accuracy: 1.0000\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.0139e-06 - accuracy: 1.0000 - val_loss: 2.7265e-07 - val_accuracy: 1.0000\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.1920e-06 - accuracy: 1.0000 - val_loss: 2.7246e-07 - val_accuracy: 1.0000\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.4746e-06 - accuracy: 1.0000 - val_loss: 2.7227e-07 - val_accuracy: 1.0000\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.1971e-06 - accuracy: 1.0000 - val_loss: 2.7208e-07 - val_accuracy: 1.0000\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.1826e-06 - accuracy: 1.0000 - val_loss: 2.7189e-07 - val_accuracy: 1.0000\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.9913e-06 - accuracy: 1.0000 - val_loss: 2.7170e-07 - val_accuracy: 1.0000\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3169e-06 - accuracy: 1.0000 - val_loss: 2.7151e-07 - val_accuracy: 1.0000\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.5298e-06 - accuracy: 1.0000 - val_loss: 2.7132e-07 - val_accuracy: 1.0000\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3108e-06 - accuracy: 1.0000 - val_loss: 2.7114e-07 - val_accuracy: 1.0000\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.6301e-06 - accuracy: 1.0000 - val_loss: 2.7094e-07 - val_accuracy: 1.0000\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.8903e-06 - accuracy: 1.0000 - val_loss: 2.7076e-07 - val_accuracy: 1.0000\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.2665e-06 - accuracy: 1.0000 - val_loss: 2.7057e-07 - val_accuracy: 1.0000\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.1625e-06 - accuracy: 1.0000 - val_loss: 2.7037e-07 - val_accuracy: 1.0000\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5327e-06 - accuracy: 1.0000 - val_loss: 2.7018e-07 - val_accuracy: 1.0000\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2785e-06 - accuracy: 1.0000 - val_loss: 2.7000e-07 - val_accuracy: 1.0000\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.5312e-06 - accuracy: 1.0000 - val_loss: 2.6980e-07 - val_accuracy: 1.0000\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3639e-06 - accuracy: 1.0000 - val_loss: 2.6961e-07 - val_accuracy: 1.0000\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7648e-06 - accuracy: 1.0000 - val_loss: 2.6942e-07 - val_accuracy: 1.0000\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.2797e-06 - accuracy: 1.0000 - val_loss: 2.6923e-07 - val_accuracy: 1.0000\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.2234e-06 - accuracy: 1.0000 - val_loss: 2.6904e-07 - val_accuracy: 1.0000\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.7950e-06 - accuracy: 1.0000 - val_loss: 2.6885e-07 - val_accuracy: 1.0000\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.4024e-06 - accuracy: 1.0000 - val_loss: 2.6865e-07 - val_accuracy: 1.0000\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.0655e-06 - accuracy: 1.0000 - val_loss: 2.6846e-07 - val_accuracy: 1.0000\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.4582e-06 - accuracy: 1.0000 - val_loss: 2.6827e-07 - val_accuracy: 1.0000\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.5875e-06 - accuracy: 1.0000 - val_loss: 2.6807e-07 - val_accuracy: 1.0000\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.1778e-06 - accuracy: 1.0000 - val_loss: 2.6788e-07 - val_accuracy: 1.0000\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9094e-06 - accuracy: 1.0000 - val_loss: 2.6769e-07 - val_accuracy: 1.0000\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.1177e-06 - accuracy: 1.0000 - val_loss: 2.6750e-07 - val_accuracy: 1.0000\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.6515e-06 - accuracy: 1.0000 - val_loss: 2.6730e-07 - val_accuracy: 1.0000\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.8886e-06 - accuracy: 1.0000 - val_loss: 2.6711e-07 - val_accuracy: 1.0000\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.6551e-06 - accuracy: 1.0000 - val_loss: 2.6691e-07 - val_accuracy: 1.0000\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.2843e-06 - accuracy: 1.0000 - val_loss: 2.6671e-07 - val_accuracy: 1.0000\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.0686e-06 - accuracy: 1.0000 - val_loss: 2.6651e-07 - val_accuracy: 1.0000\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6228e-06 - accuracy: 1.0000 - val_loss: 2.6631e-07 - val_accuracy: 1.0000\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9531e-06 - accuracy: 1.0000 - val_loss: 2.6611e-07 - val_accuracy: 1.0000\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.3095e-06 - accuracy: 1.0000 - val_loss: 2.6591e-07 - val_accuracy: 1.0000\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.4499e-06 - accuracy: 1.0000 - val_loss: 2.6570e-07 - val_accuracy: 1.0000\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.7039e-06 - accuracy: 1.0000 - val_loss: 2.6550e-07 - val_accuracy: 1.0000\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.7222e-06 - accuracy: 1.0000 - val_loss: 2.6530e-07 - val_accuracy: 1.0000\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4536e-06 - accuracy: 1.0000 - val_loss: 2.6511e-07 - val_accuracy: 1.0000\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.9394e-06 - accuracy: 1.0000 - val_loss: 2.6491e-07 - val_accuracy: 1.0000\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.5236e-06 - accuracy: 1.0000 - val_loss: 2.6471e-07 - val_accuracy: 1.0000\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.1724e-06 - accuracy: 1.0000 - val_loss: 2.6451e-07 - val_accuracy: 1.0000\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.7632e-06 - accuracy: 1.0000 - val_loss: 2.6431e-07 - val_accuracy: 1.0000\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6749e-06 - accuracy: 1.0000 - val_loss: 2.6411e-07 - val_accuracy: 1.0000\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8919e-06 - accuracy: 1.0000 - val_loss: 2.6391e-07 - val_accuracy: 1.0000\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.7565e-06 - accuracy: 1.0000 - val_loss: 2.6371e-07 - val_accuracy: 1.0000\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.8979e-06 - accuracy: 1.0000 - val_loss: 2.6350e-07 - val_accuracy: 1.0000\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.9307e-06 - accuracy: 1.0000 - val_loss: 2.6330e-07 - val_accuracy: 1.0000\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.1287e-06 - accuracy: 1.0000 - val_loss: 2.6310e-07 - val_accuracy: 1.0000\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.4711e-06 - accuracy: 1.0000 - val_loss: 2.6290e-07 - val_accuracy: 1.0000\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.1362e-06 - accuracy: 1.0000 - val_loss: 2.6270e-07 - val_accuracy: 1.0000\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6546e-06 - accuracy: 1.0000 - val_loss: 2.6250e-07 - val_accuracy: 1.0000\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.0731e-06 - accuracy: 1.0000 - val_loss: 2.6230e-07 - val_accuracy: 1.0000\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.5132e-06 - accuracy: 1.0000 - val_loss: 2.6210e-07 - val_accuracy: 1.0000\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.9254e-06 - accuracy: 1.0000 - val_loss: 2.6190e-07 - val_accuracy: 1.0000\n",
      "Epoch 231/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.9473e-06 - accuracy: 1.0000 - val_loss: 2.6169e-07 - val_accuracy: 1.0000\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.3558e-06 - accuracy: 1.0000 - val_loss: 2.6148e-07 - val_accuracy: 1.0000\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.0306e-06 - accuracy: 1.0000 - val_loss: 2.6127e-07 - val_accuracy: 1.0000\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.4170e-06 - accuracy: 1.0000 - val_loss: 2.6106e-07 - val_accuracy: 1.0000\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.2015e-06 - accuracy: 1.0000 - val_loss: 2.6086e-07 - val_accuracy: 1.0000\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.0401e-06 - accuracy: 1.0000 - val_loss: 2.6066e-07 - val_accuracy: 1.0000\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.9502e-06 - accuracy: 1.0000 - val_loss: 2.6046e-07 - val_accuracy: 1.0000\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.7244e-06 - accuracy: 1.0000 - val_loss: 2.6026e-07 - val_accuracy: 1.0000\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.1136e-06 - accuracy: 1.0000 - val_loss: 2.6006e-07 - val_accuracy: 1.0000\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.0506e-06 - accuracy: 1.0000 - val_loss: 2.5985e-07 - val_accuracy: 1.0000\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6871e-06 - accuracy: 1.0000 - val_loss: 2.5964e-07 - val_accuracy: 1.0000\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.9531e-06 - accuracy: 1.0000 - val_loss: 2.5944e-07 - val_accuracy: 1.0000\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.5139e-06 - accuracy: 1.0000 - val_loss: 2.5924e-07 - val_accuracy: 1.0000\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.4514e-06 - accuracy: 1.0000 - val_loss: 2.5904e-07 - val_accuracy: 1.0000\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.0545e-06 - accuracy: 1.0000 - val_loss: 2.5884e-07 - val_accuracy: 1.0000\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0192e-05 - accuracy: 1.0000 - val_loss: 2.5862e-07 - val_accuracy: 1.0000\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.2819e-06 - accuracy: 1.0000 - val_loss: 2.5842e-07 - val_accuracy: 1.0000\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.0186e-06 - accuracy: 1.0000 - val_loss: 2.5821e-07 - val_accuracy: 1.0000\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7995e-06 - accuracy: 1.0000 - val_loss: 2.5800e-07 - val_accuracy: 1.0000\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5358e-06 - accuracy: 1.0000 - val_loss: 2.5780e-07 - val_accuracy: 1.0000\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.6206e-06 - accuracy: 1.0000 - val_loss: 2.5760e-07 - val_accuracy: 1.0000\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.8340e-06 - accuracy: 1.0000 - val_loss: 2.5740e-07 - val_accuracy: 1.0000\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5186e-06 - accuracy: 1.0000 - val_loss: 2.5721e-07 - val_accuracy: 1.0000\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.9133e-06 - accuracy: 1.0000 - val_loss: 2.5701e-07 - val_accuracy: 1.0000\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6251e-06 - accuracy: 1.0000 - val_loss: 2.5681e-07 - val_accuracy: 1.0000\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.4309e-06 - accuracy: 1.0000 - val_loss: 2.5660e-07 - val_accuracy: 1.0000\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.7005e-06 - accuracy: 1.0000 - val_loss: 2.5640e-07 - val_accuracy: 1.0000\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.4942e-06 - accuracy: 1.0000 - val_loss: 2.5620e-07 - val_accuracy: 1.0000\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.2775e-06 - accuracy: 1.0000 - val_loss: 2.5599e-07 - val_accuracy: 1.0000\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.6251e-06 - accuracy: 1.0000 - val_loss: 2.5578e-07 - val_accuracy: 1.0000\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.1739e-06 - accuracy: 1.0000 - val_loss: 2.5558e-07 - val_accuracy: 1.0000\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.8541e-06 - accuracy: 1.0000 - val_loss: 2.5537e-07 - val_accuracy: 1.0000\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.3619e-06 - accuracy: 1.0000 - val_loss: 2.5517e-07 - val_accuracy: 1.0000\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.7200e-06 - accuracy: 1.0000 - val_loss: 2.5496e-07 - val_accuracy: 1.0000\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.5990e-06 - accuracy: 1.0000 - val_loss: 2.5476e-07 - val_accuracy: 1.0000\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6027e-06 - accuracy: 1.0000 - val_loss: 2.5455e-07 - val_accuracy: 1.0000\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.2057e-06 - accuracy: 1.0000 - val_loss: 2.5435e-07 - val_accuracy: 1.0000\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.5506e-06 - accuracy: 1.0000 - val_loss: 2.5414e-07 - val_accuracy: 1.0000\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.3470e-06 - accuracy: 1.0000 - val_loss: 2.5393e-07 - val_accuracy: 1.0000\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.8409e-06 - accuracy: 1.0000 - val_loss: 2.5372e-07 - val_accuracy: 1.0000\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.9159e-06 - accuracy: 1.0000 - val_loss: 2.5351e-07 - val_accuracy: 1.0000\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.6092e-06 - accuracy: 1.0000 - val_loss: 2.5330e-07 - val_accuracy: 1.0000\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.9043e-06 - accuracy: 1.0000 - val_loss: 2.5309e-07 - val_accuracy: 1.0000\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.4593e-06 - accuracy: 1.0000 - val_loss: 2.5288e-07 - val_accuracy: 1.0000\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.2212e-06 - accuracy: 1.0000 - val_loss: 2.5266e-07 - val_accuracy: 1.0000\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.8018e-06 - accuracy: 1.0000 - val_loss: 2.5244e-07 - val_accuracy: 1.0000\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.7311e-06 - accuracy: 1.0000 - val_loss: 2.5223e-07 - val_accuracy: 1.0000\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9626e-06 - accuracy: 1.0000 - val_loss: 2.5202e-07 - val_accuracy: 1.0000\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3261e-06 - accuracy: 1.0000 - val_loss: 2.5181e-07 - val_accuracy: 1.0000\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.6682e-06 - accuracy: 1.0000 - val_loss: 2.5160e-07 - val_accuracy: 1.0000\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5048e-06 - accuracy: 1.0000 - val_loss: 2.5139e-07 - val_accuracy: 1.0000\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.3614e-06 - accuracy: 1.0000 - val_loss: 2.5118e-07 - val_accuracy: 1.0000\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.4944e-06 - accuracy: 1.0000 - val_loss: 2.5097e-07 - val_accuracy: 1.0000\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.9757e-06 - accuracy: 1.0000 - val_loss: 2.5076e-07 - val_accuracy: 1.0000\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.2445e-06 - accuracy: 1.0000 - val_loss: 2.5055e-07 - val_accuracy: 1.0000\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.0801e-06 - accuracy: 1.0000 - val_loss: 2.5034e-07 - val_accuracy: 1.0000\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.6417e-06 - accuracy: 1.0000 - val_loss: 2.5013e-07 - val_accuracy: 1.0000\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3355e-06 - accuracy: 1.0000 - val_loss: 2.4993e-07 - val_accuracy: 1.0000\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.8839e-06 - accuracy: 1.0000 - val_loss: 2.4973e-07 - val_accuracy: 1.0000\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5296e-06 - accuracy: 1.0000 - val_loss: 2.4952e-07 - val_accuracy: 1.0000\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.8168e-06 - accuracy: 1.0000 - val_loss: 2.4933e-07 - val_accuracy: 1.0000\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8062e-06 - accuracy: 1.0000 - val_loss: 2.4913e-07 - val_accuracy: 1.0000\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5868e-06 - accuracy: 1.0000 - val_loss: 2.4893e-07 - val_accuracy: 1.0000\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.7150e-06 - accuracy: 1.0000 - val_loss: 2.4873e-07 - val_accuracy: 1.0000\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.2058e-06 - accuracy: 1.0000 - val_loss: 2.4852e-07 - val_accuracy: 1.0000\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.2693e-06 - accuracy: 1.0000 - val_loss: 2.4832e-07 - val_accuracy: 1.0000\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5401e-06 - accuracy: 1.0000 - val_loss: 2.4812e-07 - val_accuracy: 1.0000\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.7666e-06 - accuracy: 1.0000 - val_loss: 2.4792e-07 - val_accuracy: 1.0000\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.2770e-06 - accuracy: 1.0000 - val_loss: 2.4772e-07 - val_accuracy: 1.0000\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.0924e-06 - accuracy: 1.0000 - val_loss: 2.4752e-07 - val_accuracy: 1.0000\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.8433e-06 - accuracy: 1.0000 - val_loss: 2.4731e-07 - val_accuracy: 1.0000\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.2923e-06 - accuracy: 1.0000 - val_loss: 2.4711e-07 - val_accuracy: 1.0000\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.1326e-06 - accuracy: 1.0000 - val_loss: 2.4690e-07 - val_accuracy: 1.0000\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.8705e-06 - accuracy: 1.0000 - val_loss: 2.4669e-07 - val_accuracy: 1.0000\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.5294e-06 - accuracy: 1.0000 - val_loss: 2.4647e-07 - val_accuracy: 1.0000\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.7122e-06 - accuracy: 1.0000 - val_loss: 2.4625e-07 - val_accuracy: 1.0000\n",
      "Epoch 307/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6609e-06 - accuracy: 1.0000 - val_loss: 2.4603e-07 - val_accuracy: 1.0000\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.3924e-06 - accuracy: 1.0000 - val_loss: 2.4582e-07 - val_accuracy: 1.0000\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4902e-06 - accuracy: 1.0000 - val_loss: 2.4561e-07 - val_accuracy: 1.0000\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.9550e-06 - accuracy: 1.0000 - val_loss: 2.4540e-07 - val_accuracy: 1.0000\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.2428e-06 - accuracy: 1.0000 - val_loss: 2.4519e-07 - val_accuracy: 1.0000\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.6132e-06 - accuracy: 1.0000 - val_loss: 2.4499e-07 - val_accuracy: 1.0000\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.6120e-06 - accuracy: 1.0000 - val_loss: 2.4479e-07 - val_accuracy: 1.0000\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.8432e-06 - accuracy: 1.0000 - val_loss: 2.4459e-07 - val_accuracy: 1.0000\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.7586e-06 - accuracy: 1.0000 - val_loss: 2.4439e-07 - val_accuracy: 1.0000\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.2370e-06 - accuracy: 1.0000 - val_loss: 2.4420e-07 - val_accuracy: 1.0000\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6899e-06 - accuracy: 1.0000 - val_loss: 2.4400e-07 - val_accuracy: 1.0000\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.3200e-06 - accuracy: 1.0000 - val_loss: 2.4380e-07 - val_accuracy: 1.0000\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.7576e-06 - accuracy: 1.0000 - val_loss: 2.4361e-07 - val_accuracy: 1.0000\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2574e-06 - accuracy: 1.0000 - val_loss: 2.4341e-07 - val_accuracy: 1.0000\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.4381e-06 - accuracy: 1.0000 - val_loss: 2.4321e-07 - val_accuracy: 1.0000\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.4498e-06 - accuracy: 1.0000 - val_loss: 2.4301e-07 - val_accuracy: 1.0000\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.3424e-06 - accuracy: 1.0000 - val_loss: 2.4281e-07 - val_accuracy: 1.0000\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.7481e-06 - accuracy: 1.0000 - val_loss: 2.4261e-07 - val_accuracy: 1.0000\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6160e-06 - accuracy: 1.0000 - val_loss: 2.4241e-07 - val_accuracy: 1.0000\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.3962e-06 - accuracy: 1.0000 - val_loss: 2.4221e-07 - val_accuracy: 1.0000\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.4078e-06 - accuracy: 1.0000 - val_loss: 2.4202e-07 - val_accuracy: 1.0000\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.9655e-06 - accuracy: 1.0000 - val_loss: 2.4182e-07 - val_accuracy: 1.0000\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.3281e-06 - accuracy: 1.0000 - val_loss: 2.4163e-07 - val_accuracy: 1.0000\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.7825e-06 - accuracy: 1.0000 - val_loss: 2.4144e-07 - val_accuracy: 1.0000\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.8783e-06 - accuracy: 1.0000 - val_loss: 2.4124e-07 - val_accuracy: 1.0000\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.0712e-06 - accuracy: 1.0000 - val_loss: 2.4105e-07 - val_accuracy: 1.0000\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.9920e-06 - accuracy: 1.0000 - val_loss: 2.4084e-07 - val_accuracy: 1.0000\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.5778e-06 - accuracy: 1.0000 - val_loss: 2.4065e-07 - val_accuracy: 1.0000\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.9936e-06 - accuracy: 1.0000 - val_loss: 2.4044e-07 - val_accuracy: 1.0000\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.1602e-06 - accuracy: 1.0000 - val_loss: 2.4023e-07 - val_accuracy: 1.0000\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.3249e-06 - accuracy: 1.0000 - val_loss: 2.4002e-07 - val_accuracy: 1.0000\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.0389e-06 - accuracy: 1.0000 - val_loss: 2.3980e-07 - val_accuracy: 1.0000\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.4030e-06 - accuracy: 1.0000 - val_loss: 2.3960e-07 - val_accuracy: 1.0000\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.1076e-06 - accuracy: 1.0000 - val_loss: 2.3939e-07 - val_accuracy: 1.0000\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.2816e-06 - accuracy: 1.0000 - val_loss: 2.3918e-07 - val_accuracy: 1.0000\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.0339e-06 - accuracy: 1.0000 - val_loss: 2.3897e-07 - val_accuracy: 1.0000\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.4047e-06 - accuracy: 1.0000 - val_loss: 2.3876e-07 - val_accuracy: 1.0000\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.6146e-06 - accuracy: 1.0000 - val_loss: 2.3854e-07 - val_accuracy: 1.0000\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.2656e-06 - accuracy: 1.0000 - val_loss: 2.3833e-07 - val_accuracy: 1.0000\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.9114e-06 - accuracy: 1.0000 - val_loss: 2.3811e-07 - val_accuracy: 1.0000\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.9962e-06 - accuracy: 1.0000 - val_loss: 2.3790e-07 - val_accuracy: 1.0000\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.0710e-06 - accuracy: 1.0000 - val_loss: 2.3769e-07 - val_accuracy: 1.0000\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2046e-06 - accuracy: 1.0000 - val_loss: 2.3747e-07 - val_accuracy: 1.0000\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.8561e-06 - accuracy: 1.0000 - val_loss: 2.3726e-07 - val_accuracy: 1.0000\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.4321e-06 - accuracy: 1.0000 - val_loss: 2.3705e-07 - val_accuracy: 1.0000\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.3026e-06 - accuracy: 1.0000 - val_loss: 2.3685e-07 - val_accuracy: 1.0000\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.0598e-06 - accuracy: 1.0000 - val_loss: 2.3665e-07 - val_accuracy: 1.0000\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.4045e-06 - accuracy: 1.0000 - val_loss: 2.3645e-07 - val_accuracy: 1.0000\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7251e-06 - accuracy: 1.0000 - val_loss: 2.3625e-07 - val_accuracy: 1.0000\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.0515e-06 - accuracy: 1.0000 - val_loss: 2.3604e-07 - val_accuracy: 1.0000\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.1506e-06 - accuracy: 1.0000 - val_loss: 2.3584e-07 - val_accuracy: 1.0000\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5441e-06 - accuracy: 1.0000 - val_loss: 2.3564e-07 - val_accuracy: 1.0000\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.1397e-06 - accuracy: 1.0000 - val_loss: 2.3543e-07 - val_accuracy: 1.0000\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8669e-06 - accuracy: 1.0000 - val_loss: 2.3523e-07 - val_accuracy: 1.0000\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.9876e-06 - accuracy: 1.0000 - val_loss: 2.3502e-07 - val_accuracy: 1.0000\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.7122e-06 - accuracy: 1.0000 - val_loss: 2.3481e-07 - val_accuracy: 1.0000\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.8111e-06 - accuracy: 1.0000 - val_loss: 2.3461e-07 - val_accuracy: 1.0000\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.7549e-06 - accuracy: 1.0000 - val_loss: 2.3440e-07 - val_accuracy: 1.0000\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.9047e-06 - accuracy: 1.0000 - val_loss: 2.3419e-07 - val_accuracy: 1.0000\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.7427e-06 - accuracy: 1.0000 - val_loss: 2.3399e-07 - val_accuracy: 1.0000\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.6756e-06 - accuracy: 1.0000 - val_loss: 2.3378e-07 - val_accuracy: 1.0000\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6020e-06 - accuracy: 1.0000 - val_loss: 2.3357e-07 - val_accuracy: 1.0000\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.6409e-06 - accuracy: 1.0000 - val_loss: 2.3336e-07 - val_accuracy: 1.0000\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3082e-06 - accuracy: 1.0000 - val_loss: 2.3316e-07 - val_accuracy: 1.0000\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0255e-06 - accuracy: 1.0000 - val_loss: 2.3295e-07 - val_accuracy: 1.0000\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.9560e-06 - accuracy: 1.0000 - val_loss: 2.3274e-07 - val_accuracy: 1.0000\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.8879e-06 - accuracy: 1.0000 - val_loss: 2.3253e-07 - val_accuracy: 1.0000\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0822e-06 - accuracy: 1.0000 - val_loss: 2.3232e-07 - val_accuracy: 1.0000\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5365e-06 - accuracy: 1.0000 - val_loss: 2.3211e-07 - val_accuracy: 1.0000\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0711e-06 - accuracy: 1.0000 - val_loss: 2.3191e-07 - val_accuracy: 1.0000\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.2516e-06 - accuracy: 1.0000 - val_loss: 2.3170e-07 - val_accuracy: 1.0000\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.7085e-06 - accuracy: 1.0000 - val_loss: 2.3150e-07 - val_accuracy: 1.0000\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.7898e-06 - accuracy: 1.0000 - val_loss: 2.3130e-07 - val_accuracy: 1.0000\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.1256e-06 - accuracy: 1.0000 - val_loss: 2.3109e-07 - val_accuracy: 1.0000\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.6424e-06 - accuracy: 1.0000 - val_loss: 2.3089e-07 - val_accuracy: 1.0000\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.4992e-06 - accuracy: 1.0000 - val_loss: 2.3069e-07 - val_accuracy: 1.0000\n",
      "Epoch 383/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.0308e-06 - accuracy: 1.0000 - val_loss: 2.3048e-07 - val_accuracy: 1.0000\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.9035e-06 - accuracy: 1.0000 - val_loss: 2.3028e-07 - val_accuracy: 1.0000\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.2386e-06 - accuracy: 1.0000 - val_loss: 2.3008e-07 - val_accuracy: 1.0000\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7718e-06 - accuracy: 1.0000 - val_loss: 2.2988e-07 - val_accuracy: 1.0000\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4837e-06 - accuracy: 1.0000 - val_loss: 2.2969e-07 - val_accuracy: 1.0000\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.7724e-06 - accuracy: 1.0000 - val_loss: 2.2949e-07 - val_accuracy: 1.0000\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.2021e-06 - accuracy: 1.0000 - val_loss: 2.2929e-07 - val_accuracy: 1.0000\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.5512e-06 - accuracy: 1.0000 - val_loss: 2.2909e-07 - val_accuracy: 1.0000\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.1841e-06 - accuracy: 1.0000 - val_loss: 2.2888e-07 - val_accuracy: 1.0000\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3631e-06 - accuracy: 1.0000 - val_loss: 2.2869e-07 - val_accuracy: 1.0000\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.7874e-06 - accuracy: 1.0000 - val_loss: 2.2848e-07 - val_accuracy: 1.0000\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.6424e-06 - accuracy: 1.0000 - val_loss: 2.2827e-07 - val_accuracy: 1.0000\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.6436e-06 - accuracy: 1.0000 - val_loss: 2.2807e-07 - val_accuracy: 1.0000\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.3939e-06 - accuracy: 1.0000 - val_loss: 2.2786e-07 - val_accuracy: 1.0000\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.0994e-06 - accuracy: 1.0000 - val_loss: 2.2765e-07 - val_accuracy: 1.0000\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.1861e-06 - accuracy: 1.0000 - val_loss: 2.2744e-07 - val_accuracy: 1.0000\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.5682e-06 - accuracy: 1.0000 - val_loss: 2.2722e-07 - val_accuracy: 1.0000\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5405e-06 - accuracy: 1.0000 - val_loss: 2.2701e-07 - val_accuracy: 1.0000\n",
      "gru+attention模型训练完成\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7f2df5b211f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_isenzyme(x_train,y_train,x_vali,y_vali,ecs,model_file=cfg.ISENZYME_MODEL, force_model_update=cfg.UPDATE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 loading task data\n",
      "step 2: Loading features, embdding method=esm32\n",
      "step 3: train isEnzyme model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'config' has no attribute 'ISENZYME_LABEL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m task1_X \u001b[39m=\u001b[39m task1_X\u001b[39m.\u001b[39miloc[:\u001b[39m100\u001b[39m,:]\n\u001b[1;32m     20\u001b[0m x_train,y_train,x_vali,y_vali,ecs \u001b[39m=\u001b[39m data_process(task1_X,\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m labelsave(ecs,labefile\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39;49mISENZYME_LABEL)\n\u001b[1;32m     22\u001b[0m train_isenzyme(x_train,y_train,x_vali,y_vali,ecs,model_file\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mISENZYME_MODEL, force_model_update\u001b[39m=\u001b[39mcfg\u001b[39m.\u001b[39mUPDATE_MODEL)\n\u001b[1;32m     25\u001b[0m \u001b[39m#4. task2 train\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'config' has no attribute 'ISENZYME_LABEL'"
     ]
    }
   ],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    \n",
    "    EMBEDDING_METHOD = 'esm32'\n",
    "\n",
    "    # 1. 读入数据\n",
    "    print('step 1 loading task data')\n",
    "    data_task1_train = pd.read_feather(cfg.FILE_TASK1_TRAIN)\n",
    "    data_task2_train = pd.read_feather(cfg.FILE_TASK2_TRAIN)\n",
    "    data_task3_train = pd.read_feather(cfg.FILE_TASK3_TRAIN)\n",
    "    data_task3_test = pd.read_feather(cfg.FILE_TASK3_TEST_2019)\n",
    "\n",
    "    # # 2. 读取特征\n",
    "    print(f'step 2: Loading features, embdding method={EMBEDDING_METHOD}')\n",
    "    feature_df = bcommon.load_data_embedding(embedding_type=EMBEDDING_METHOD)\n",
    "\n",
    "    #3. task1 train\n",
    "    print('step 3: train isEnzyme model')\n",
    "    task1_X = get_train_X_Y(traindata=data_task1_train, feature_bankfile=feature_df, task=1)\n",
    "    task1_X = task1_X.iloc[:100,:]\n",
    "    x_train,y_train,x_vali,y_vali,ecs = data_process(task1_X,1)\n",
    "    labelsave(ecs,labefile=cfg.ISENZYME_LABEL)\n",
    "    train_isenzyme(x_train,y_train,x_vali,y_vali,ecs,model_file=cfg.ISENZYME_MODEL, force_model_update=cfg.UPDATE_MODEL)\n",
    "\n",
    "\n",
    "    #4. task2 train\n",
    "    print('step 4: train how many enzymes model')\n",
    "    task2_X= get_train_X_Y(traindata=data_task2_train, feature_bankfile=feature_df, task=2)\n",
    "    task2_X = task2_X.iloc[:100,:]\n",
    "    x_train,y_train,x_vali,y_vali,ecs = data_process(task2_X,2)\n",
    "    labelsave(ecs,labefile=cfg.HOWMANY_LABEL)\n",
    "    train_howmany_enzyme(x_train,y_train,x_vali,y_vali,ecs,model_file=cfg.HOWMANY_MODEL, force_model_update=cfg.UPDATE_MODEL)\n",
    "\n",
    "\n",
    "    #4. 加载EC号训练数据\n",
    "    print('step 5:loading ec to label dict')\n",
    "    task3_X = get_train_X_Y(traindata=data_task3_train, feature_bankfile=feature_df, task=3, task3_test=data_task3_test)\n",
    "    task3_X = task3_X.iloc[:100,:]\n",
    "    x_train,y_train,x_vali,y_vali,ecs = data_process(task3_X,3)\n",
    "    labelsave(ecs,labefile=cfg.EC_LABEL)\n",
    "    train_ec_gru(x_train,y_train,x_vali,y_vali,ecs,model_file=cfg.EC_MODEL, force_model_update=cfg.UPDATE_MODEL)\n",
    "    \n",
    "\n",
    "    # # 2. 写入fasta文件\n",
    "\n",
    "    # table2fasta(table=train,file_out=cfg.TRAIN_FASTA)\n",
    "    # table2fasta(table=test, file_out=cfg.TEST_FASTA)\n",
    "    # #7. 创建blast比对数据库\n",
    "    # bcommon.make_diamond_db(dbtable=train.iloc[:, np.r_[0,5]],to_db_file=cfg.FILE_BLAST_TRAIN_DB) # 创建是否是酶blast数据库\n",
    "\n",
    "    print('train finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMLF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b0f740237ba4768c544d9b9677983e49b45ca1230fda464ede0b93eba99c7d2"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
